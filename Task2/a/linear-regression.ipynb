{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2242,"sourceType":"datasetVersion","datasetId":1256}],"dockerImageVersionId":30260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2023-09-29T06:11:59.203994Z","iopub.execute_input":"2023-09-29T06:11:59.204488Z","iopub.status.idle":"2023-09-29T06:11:59.210282Z","shell.execute_reply.started":"2023-09-29T06:11:59.20445Z","shell.execute_reply":"2023-09-29T06:11:59.208899Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T06:12:00.077032Z","iopub.execute_input":"2023-09-29T06:12:00.078031Z","iopub.status.idle":"2023-09-29T06:12:00.091153Z","shell.execute_reply.started":"2023-09-29T06:12:00.077975Z","shell.execute_reply":"2023-09-29T06:12:00.089876Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"plot the data here","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Standardize the data\n    fill this\n### Why Use Standardization in Machine Learning?\n fill this\n\n### How to Standardize Data\n\n    fill this","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reshaping data for the correct shape for the model","metadata":{}},{"cell_type":"markdown","source":"why cant we make the model without reshaping?","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Implementation","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Linear Regression Model\n\nLinear regression is a fundamental model in machine learning used for predicting a continuous output variable based on input features. The model function for linear regression is represented as:\n\n$$f_{w,b}(x) = wx + b$$\n\nIn this equation, $f_{w,b}(x)$ represents the predicted output, $w$ is the weight parameter, $b$ is the bias parameter, and $x$ is the input feature.\n\n## Model Training\n\nTo train a linear regression model, we aim to find the best values for the parameters $(w, b)$ that best fit our dataset.\n\n### Forward Pass\n\nThe forward pass is a step where we compute the linear regression output for the input data $X$ using the current weights and biases. It's essentially applying our model to the input data.\n\n### Cost Function\n\nThe cost function is used to measure how well our model is performing. It quantifies the difference between the predicted values and the actual values in our dataset. The cost function is defined as:\n\n$$J(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)})^2$$\n\nHere, $J(w, b)$ is the cost, $m$ is the number of training examples, $x^{(i)}$ is the input data for the $i$-th example, $y^{(i)}$ is the actual output for the $i$-th example, and $w$ and $b$ are the weight and bias parameters, respectively.\n\n### Backward Pass (Gradient Computation)\n\nThe backward pass computes the gradients of the cost function with respect to the weights and biases. These gradients are crucial for updating the model parameters during training. The gradient formulas are as follows:\n\n$$\n\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{m} \\sum_{i=0}^{m-1} (f_{w,b}(X^{(i)}) - y^{(i)})\n$$\n\n$$\n\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{m} \\sum_{i=0}^{m-1} (f_{w,b}(X^{(i)}) - y^{(i)})X^{(i)}\n$$\n\n## Training Process\n\nThe training process involves iteratively updating the weights and biases to minimize the cost function. This is typically done through an optimization algorithm like gradient descent. The update equations for parameters are:\n\n$$w \\leftarrow w - \\alpha \\frac{\\partial J}{\\partial w}$$\n\n$$b \\leftarrow b - \\alpha \\frac{\\partial J}{\\partial b}$$\n\nHere, $\\alpha$ represents the learning rate, which controls the step size during parameter updates.\n\nBy iteratively performing the forward pass, computing the cost, performing the backward pass, and updating the parameters, the model learns to make better predictions and fit the data.\n","metadata":{}},{"cell_type":"code","source":"class LinearRegression:\n    \n    def __init__()):\n        pass\n        \n\n    def initialize_parameters():\n        pass \n\n    def forward():\n        pass\n\n    def compute_cost():\n        pass\n\n    def backward():\n        pass\n\n    def fit(self, X, y, iterations, plot_cost=True):\n       \n        pass\n        \n    def predict(self, X):\n        pass\n    \n\n    def save_model(self, filename=None):\n    \n    @classmethod\n    def load_model(cls, filename):\n        pass\n\n   ","metadata":{"execution":{"iopub.status.busy":"2023-09-29T06:12:02.773435Z","iopub.execute_input":"2023-09-29T06:12:02.773928Z","iopub.status.idle":"2023-09-29T06:12:02.796668Z","shell.execute_reply.started":"2023-09-29T06:12:02.773887Z","shell.execute_reply":"2023-09-29T06:12:02.795613Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lr = LinearRegression(??)\nlr.fit(X_train, y_train, ??)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T06:12:04.222072Z","iopub.execute_input":"2023-09-29T06:12:04.223332Z","iopub.status.idle":"2023-09-29T06:12:04.309693Z","shell.execute_reply.started":"2023-09-29T06:12:04.223271Z","shell.execute_reply":"2023-09-29T06:12:04.308476Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lr.save_model('model.pkl')","metadata":{"execution":{"iopub.status.busy":"2023-09-29T06:12:05.415969Z","iopub.execute_input":"2023-09-29T06:12:05.417361Z","iopub.status.idle":"2023-09-29T06:12:05.424426Z","shell.execute_reply.started":"2023-09-29T06:12:05.417298Z","shell.execute_reply":"2023-09-29T06:12:05.422688Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation\n\n","metadata":{}},{"cell_type":"markdown","source":"### 1. Mean Squared Error (MSE)\n\n**Formula:**\n$$\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{true}_i} - y_{\\text{pred}_i})^2\n$$\n\n**Description:**\n - fill this\n\n**Interpretation:**\n-  fill this\n\n### 2. Root Mean Squared Error (RMSE)\n\n**Formula:**\n$$\n\\text{RMSE} = \\sqrt{\\text{MSE}}\n$$\n\n**Description:**\n- fill this\n**Interpretation:**\n- fill this\n\n\n### 3. R-squared ($R^2$)\n\n**Formula:**\n$$\nR^2 = 1 - \\frac{\\text{SSR}}{\\text{SST}}\n$$\n\n**Description:**\n fill this \n\n**Interpretation:**\n fill this\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RegressionMetrics:\n    @staticmethod\n    def mean_squared_error(y_true, y_pred):\n        \"\"\"\n        Calculate the Mean Squared Error (MSE).\n\n        Args:\n            y_true (numpy.ndarray): The true target values.\n            y_pred (numpy.ndarray): The predicted target values.\n\n        Returns:\n            float: The Mean Squared Error.\n        \"\"\"\n        \n    @staticmethod\n    def root_mean_squared_error(y_true, y_pred):\n        \"\"\"\n        Calculate the Root Mean Squared Error (RMSE).\n\n        Args:\n            y_true (numpy.ndarray): The true target values.\n            y_pred (numpy.ndarray): The predicted target values.\n\n        Returns:\n            float: The Root Mean Squared Error.\n        \"\"\"\n        \n    @staticmethod\n    def r_squared(y_true, y_pred):\n        pass\n        ","metadata":{"execution":{"iopub.status.busy":"2023-09-29T05:56:51.090284Z","iopub.execute_input":"2023-09-29T05:56:51.090944Z","iopub.status.idle":"2023-09-29T05:56:51.100421Z","shell.execute_reply.started":"2023-09-29T05:56:51.090872Z","shell.execute_reply":"2023-09-29T05:56:51.099383Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model.predict(X_test)\nmse_value = RegressionMetrics.mean_squared_error(y_test, y_pred)\nrmse_value = RegressionMetrics.root_mean_squared_error(y_test, y_pred)\nr_squared_value = RegressionMetrics.r_squared(y_test, y_pred)\n\nprint(f\"Mean Squared Error (MSE): {mse_value}\")\nprint(f\"Root Mean Squared Error (RMSE): {rmse_value}\")\nprint(f\"R-squared (Coefficient of Determination): {r_squared_value}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T05:54:52.575962Z","iopub.execute_input":"2023-09-29T05:54:52.576446Z","iopub.status.idle":"2023-09-29T05:54:52.584259Z","shell.execute_reply.started":"2023-09-29T05:54:52.576404Z","shell.execute_reply":"2023-09-29T05:54:52.58316Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}